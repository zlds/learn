Hardware:
  - F5
  - BigIP
  - IBM A10
  - Citrix Netscaler

Software
  - Lvs
  - Haproxy
  - Nginx


LVS在各大公司的使用情况是：
百度 – BVS（现淘宝的 普空 之前在百度搞得类似fullnat项目，未开源） 结构是fullnat模式+ospf等价多路径
阿里 – LVS（fullnat模式，已开源）结构是fullnat模式+ospf等价多路径，通过交换机ospf的一直性hash来避免session不一致的情况
腾讯 – TWG（也是类似于fullnat的项目，未开源）结构是fullnat模式+ospf等价多路径，通过LVS集群定期同步来实现session一致

DAS(直接附加存储)
  连接到主板总线上
NAS(网络附加存储)
 文件共享服务器（NFS）文件及别，性能比较低
SAN（存储区域网络） G
（scsi）
DMA：Direct Memory Access
USB
IDE：100M
SATA 300 SATA2 600M
SCSI 300
SAS  Gp
对于计算机来讲，是4种不同的总线Bus。通过桥的设备连接起来。数据传输介质



横向扩展(Scale-out) 服务器数量增加
纵向扩展(Scale-up)  提升单个服务器性能

LVS(Linux Virtual Server)：由章文嵩开发的一款自由软件。可以实现高可用，负载均衡。
Lvs服务器集群由3个部分组成：
  - 最前端是的负载均衡层(这里用Load Blalncer表示)，
  - 中间是服务器群组层(用Server Array表示)，
  - 低端是数据共享存储层(用Shared Storage表示)。

负载均衡：位于整个集群系统的最前端，由一台或多台负载调度器(Director Server)组成。
 lvs的核心模块IPVS就安装在Dircetor Server上，在Director Server上还要安装对Real Server的监控模块Ldircetord，此模块
 用于监测各个Real Server服务的健康状态。在Real Server不可用时可以将其从LVS路由表中剔除，在恢复时重新加入。

服务器群组：由一组实际运行应用服务的机器组成，Real Server可以是web服务器，mail，ftp，dns等等。每个real server之间
通过高速的Lan或分布在各地的wan相连接。
共享存储：是为所有real server提供共享存储空间和内容一致性的存储区域，一般有磁盘阵列设备组成。


IPVS实现负载均衡的方式有4种，分别是NAT,DR,TUN,FULLNAT。

LVS工作模式
      NAT: 网络地址转换
          client -> vs -> Rs ->vs ->client
          in 进行DNAT转换
          out 进行SNAT转换
          当用户请求到达调度器时，调度器将请求报文的目标地址(即虚拟IP地址)改写选定的Real Server地址，同时将报文的目标端口也改成选定的Real端口

      DR: 直接路由
          client -> vs -> Rs ->client
          必须在同一个VLAN下面
          LVS和RS集群都要绑定VIP
          DR模式下、数据分发过程中不修改IP地址、只修改mac地址。
          VS/DR通过修改请求报文的MAC地址，将请求发送到Real Server，Real Server直接返回给客户端
          
      TUN: IP隧道模式  
          client ->vs ->Rs -> client
          可以跨越不同的网络 
          lvs和rs都要绑定vip
          对原始数据包进行一层封装、然后在转发给Rs、rs接收之后进行解封装、然后得到原始请求。

      FullNAT:
          - 类似nat模式
          - 可以跨越网络(vlan)
          - IN和OUT数据包都是经过LVS
          - 唯一区别: 后端的rs或者交换机不需要做任何配置

          in dnat+snat (目标地址改成后端real ip,源地址改成lvs设备地址)
          out snat + dnat (源地址改成lvs设备上的地址、目标地址改为真实的用户地址)

          Full-NAT由此而生、解决的是LVS和RS跨VLAN的问题、而跨VLAN问题解决后、LVS和RS不再存在VLAN上的从属关系、可以做到多个LVS对应多个RS、解决水平扩展的问题。

        LVS和交换机间运行OSPF协议、交换机上生成该VIP的等价路由ECMP  

        fullnat一个最大的问题是、rs无法取得用户ip、解决问题需通过TOA: 主要原理是将client address放到tcp option里面、带给后端rs、rs上通过toa内核模块hack了getname函数、给用户态返回tcp-option中的clientip

 NAT
1.所有的Ds和Rs必须在同一个网络中，Rip的网关是Dip
2.Rip私有地址，仅用于和DIP通信
3.D同时处理入站和出站的请求
4.Rip的网关要指向DIP
5.可以实现端口映射。
6.R可以是任意系统
7.D很有可能成为系统瓶颈。

DR：
1.必须在同一个物理网络上，不能跨越路由器。
2.rip可以使用公网ip地址。
3.d只处理入站请求，响应不在经过dip
4.r的网关不在是dip，
5.不能使用端口映射。
6.系统不限制
7.DR的模型的压力低于nat模型，就算是同样配置

TUN：
1.集群节点和Director不必在同一个网络
2.不能使用端口映射
3.只能使用那些支持IP隧道协议的操作系统做Realserver


LVS的调度算法简介
1.轮叫调度（Round Robin）
调度器通过“轮叫”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。

2.加权轮叫（Weighted Round Robin）
  - 根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。
     
3.最少链接（Least Connections）
调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。
谁不干活就给谁分配：lc - 根据最小连接数分派

4.加权最少链接（Weighted Least Connections）
在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。
带权重的谁不干活就给谁分配：wlc - 带权重的。机器配置好的权重高

5.基于局部性的最少链接（Locality-Based Least Connections）
“基于局部性的最少链接”调度算法是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。
基于地区的最少连接调度：lblc - 缓存服务器集群。基于本地的最小连接。把请求传递到负载小的服务器上

6.带复制的基于局部性最少链接（Locality-Based Least Connections with Replication）
“带复制的基于局部性最少链接”调度算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护从一个目标 IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。
带有复制调度的基于地区的最少连接调度：lblcr - 带复制调度的缓存服务器集群。某页面缓存在服务器A上，被访问次数极高，而其他缓存服务器负载较低，监视是否访问同一页面，如果是访问同一页面则把请求分到其他服务器。

7.目标地址散列（Destination Hashing）
"目标地址散列"调度算法根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。
目标散列调度：realserver中绑定两个ip。ld判断来者的ISP商，将其转到相应的IP。

8.源地址散列（Source Hashing）
“源地址散列”调度算法根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。了解这些算法原理能够在特定的应用场合选择最适合的调度算法，从而尽可能地保持Real Server的最佳利用性。当然也可以自行开发算法，不过这已超出本文范围，请参考有关算法原理的资料。
源散列调度：源地址散列。基于client地址的来源区分。（用的很少）

9. 最短的期望的延迟（Shortest Expected Delay Scheduling SED）
基于wlc算法。这个必须举例来说了ABC三台机器分别权重123 ，连接数也分别是123。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用sed算法后会进行这样一个运算A:（1+1)/1B:（1+2)/2C:（1+3)/3根据运算结果，把连接交给C 。

10.最少队列调度（Never Queue Scheduling NQ）
无需队列。如果有台realserver的连接数＝0就直接分配过去，不需要在进行sed运算。



ifconfig eth1 192.168.200.231 netmask 255.255.255.255 broadcast 192.168.200.231  配置虚拟ip。重启就没了，要写配置文件
echo "1" >/proc/sys/net/ipv4/conf/eth0/arp_ignore关闭arp
echo "2">/proc/sys/net/ipv4/conf/eth0/arp_announce
echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore关闭arp
echo "2">/proc/sys/net/ipv4/conf/all/arp_announce

arp_ignore 1 如果接收设备上没有这个ip就不做回应
arp_announce 对网络接口上发出的ARP请求包中的源地址作出相应的限制

arp_ignore 因后端real server也要绑定vip、所以要关闭real server上面vip的arp响应
arp_announce 后端realserver在处理请求后、将响应回应给客户端时、如果此时需要进行arp请求时、arp请求的源地址、使用的是发出去的请求(eth0)、而不是产生结果的接口(lo ip)、这样可以防止arp请求打乱原有的vip的mac





SHARE78 国际标准
国家标准GB/T 20988-2007


Nginx:
   模块化设计，静态加载模块，只能重新编译。Nginx的模块结构上分为核心模块，基础模块和第三方模块,HTTP,EVENT,MAIL模块等属于核心模块
   ,HTTP Access,HTTP FastCG模块,HTTP Proxy,HTTP Rewrite模块属于基本模块，
配置文件主要分成四个部分:
  - mian(全局设置)
  - server(主机设置)
  - location(匹配URL特定的位置的设置)
  - Upstream（负载均衡)。

Upstream: 通过一个调度算法来实现客户端ip到后端服务器的负载均衡。目前有4种调调算法：
  - 轮询(默认)。每个请求按时间顺序逐一分配到不同的后端服务器
  - Weight。指定轮询权值,Weight值越大,分配到的访问几率越高,主要是用于后端每台服务器性能不均的情况下(就是硬件)
  - ip_hash。每个请求按访问ip的hash几个分配,这样来自同一个ip的访问固定访问一个后台服务器,有效解决了动态网页存在的Session共享问题。
  - fair。比上面两个更智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能的进行负载均衡,也就是跟据后端服务器的响应时间来分配请求,响应时间段的优先分配。Nginx本身不支持fair的,如果需要使用这种调度算法,必须下载Nginx的upstream_fairmk。
  - url_hash。按访问url的hash结果来分配请求,使每个url定向到同一个后端服务器,可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果要使用这种调度算法,必须安装Nginx的hash软件包。
[注意 当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能是weight和backup]



日志：log_format  main  '$http_x_forwarded_for - $remote_user [$time_local] "$request" '
                      '"$request_time" "$upstream_response_time"  "$request_body" '
                      ' $body_bytes_sent "$http_referer" $status '
                      '"$http_user_agent" ';


proxy_connect_timeout 是和后端建立连接的超时时间
proxy_read_timeout 是从后端读取数据的超时时间、两次读取操作的时间间隔如果大于这个值、后端的连接会被关闭

  $request_time
    - 从请求的第一字节到发送完响应数据的时间、即包括接收请求数据时间、程序响应时间、输出响应数据时
  间。
  $upstream_response_time
    - 从nginx向后端建立连接开始到接收完数据然后关闭连接为止的时间。


last    不终止重写后的url匹配、即新的url会再从server走一遍匹配流程。
break   停止执行后续的rewrite指令集
redirect   302
permanent  301

Thundering herd problem 惊群问题：
accept_mutex:
    当一个新连接到达时、如果激活了accept_mutex、那么多个worker将以串行方式来处理、其中有一个worker会被唤醒、其他的worker继续保持睡眠状态;如果没有激活accept_mutex、那么所有的worker都会被唤醒、不过只有一个worker能获取新连接、其它的worker会重新进入睡眠状态。

accept_mutex_delay:
     默认500ms
     当启用accept_mutex、只有一个持有mutex锁的worker进程会接受并处理请求、其它worker进程等待。accept_mutex_delay指定时间就是这些worker进程的等待时间、过了等待时间下一个worker进程便取得mutex锁处理请求。

sendfile 在内核级别完成文件拷贝。
  - 传统方式: 硬盘 > kernel buffer > user buffer > kernel socket buffer > 协议栈
  - sendfile: 硬盘 > Kernel buffer > Kernel socket buffer > 协议栈


tcp_nodelay 直接发送。禁用Nagle 算法. Nginx 只会针对处于 keep-alive 状态的 TCP 连接才会启用

tcp_nopush 启用之后、数据包会累计到一定大小之后才会发送、减少了额外开销、提高网络效率。(只有启用了sendfile之后才生效)

(tcp_nopush tcp_nodelay 二者矛盾、实际上、它们确实可以一起用、最终的效果是先填满包、再尽快发送)
Nagle算法(在发出去的数据还未被确认之前、新生成的小数据先存起来、凑满一个MSS或者等到收到确认之后在发送)



TCP_NODELAY:
  - 主要是解决网络上出现大量小包问题、就像客运汽车、来一人发一辆车、效率将会很差、这就是典型的tcp小包问题、网络利用率比较低、而nagle算法是解决思路、就是将即将发送的小包、缓存和合并成一个大包、然后一次性发送出去、就像客运汽车满员发车一样、这样效率就提高了很多。


TCP Delay ACK:
  - 延迟确认机制: 是让接收方将多个收到数据包的ACK、打包成一个ACK包返回给发送方、从而提高网络传输系效率。
  - 接收方在收到数据后、并不会立即回复ACK、而是延迟一定时间、一般ACK延迟发送的时间为200ms


nginx.conf:
  - keepalive_timeout  60; 设置空闲长连接时间超时时间
  - keepalive_requests 300; #设置长连接请求数、当请求超过时、nginx关闭该连接.

upstream:
  - keepalive_timeout 50s; #设置后端长连接的最大空闲超时时间
  - keepalive 500;#设置空闲keepalive连接的最大数量、超出时最近使用最>少的连接将被关闭

proxy_connect_timeout 60s 向上游服务器建立tcp连接超时的时间
proxy_socket_keepalive on TCP连接保持时间
proxy_buffer_size default 4k|8k  接收上游的http响应头部

缓存相关:
  ETag/If-None-Match:
    - 客户端第一次请求的时候、服务器会给每个资源生成一个ETag标记。这个ETag是根据每个资源生成的唯一的hash串、资源发生变化ETag随之更改、之后将这个ETag返回给客户端。客户端将请求的资源和ETag都缓存到本地
    - ETag 被保存以后、在下次请求时会当作If-None-Match: "5d1c9a72-102"发出去、如果资源没有发生变化、返回304、浏览器使用本地缓存。

  Last-Modified/If-Modified-Since
    - 在客户端第一次请求的时候、服务器会返回资源最后的修改时间、记住Last-Modified。客户端会将这个字段和资源缓存起来。
    - Last-Modified被保存以后、下次请求时会以Last-Modified-Since字段被发送。
    - 服务端收到If-Modified-Since字段与服务器上保存的Last-Modified字段比较:
        - 如Last-Modified最后修改时间大于请求的Last-Modified-Since、说明资源被改动过、就会把资源从新返回给客户端
        - 若Last-Modified最后修改的时间等于If-Modified-Since、说明资源没有改动动、返回304.
if-modified-since:  不是通过etag、是通过时间
expires time; 设置缓存时间

Tengine: 模块
  - 一致性hash ip  和会话保持
  - 健康状态检查

- trim 模块:
    - 该模块用于删除html、内嵌javascript和css中的注释以及重复的空白符
    - 配置:
        location / {
          trim on;
          trim_js on;
          trim_css on;
        }
- ngx_http_concat_module 模块:
    - 该模块用于合并多个文件在一个响应报文中
    - 请求参数需要用两个问号('??')、如
      http://example.com/??style1.css,style2.css,foo/style3.css
    - 配置参数
      location /static/css/ {
          concat on;
          concat_max_files 20; #最多能合并的文件数量
      }  
          

vary字段:
      HTTP内容协商: 同一个URL可以提供多份不同的文档、如包含压缩和未压缩
      服务端根据客户端发送的请求头中某些字段自动发送最合适的版本
      Vary: Accept-Encoding响应头、明确告知缓存服务器按照Accept-Encoding字段的内容、分别缓存不同的版本

nginx epoll:
    LT 请求到了、epoll_wait返回后没有去处理请求、下次epoll_wait时此请求还是会返回(立刻)
    ET 这次没处理、下次epoll_wait时将不返回(所以我们应该每一次一定要处理)、可见很大程度降低了epoll的触发次数。


  rewrite:
           =  精确匹配
           ~  区分大小写
           ~* 不区分大小写
           ^~ 普通字符匹配、一般用于目录匹配。匹配之后停止向下搜索
           /  通用匹配、如果没有其它匹配、任何请求都会匹配到
           @ 定义一个location 在内部定向时
           = ---> ^~ ---> ~/~*  

           . 匹配除换行以外的任意字符
           ? 重复0次或1次
           * 重复0次或更多次
           + 重复1次或更多次
           
           \d 匹配数字
                 


如果是last loc1中有一个重写规则、它将loc1更改为loc2、则该请求将被重写并传递给loc2
如果是break 则该请求属于loc1

break:
      位于location外 匹配到之后就停止、不在解析重写条件(后续匹配)
      位于location内 不在解析新的重写条件、接续处理当前location
last:
      位于location外 匹配到之后接着进入下一个阶段。执行新的匹配。
      位于location内:
            不在解析重写条件  (当前location匹配一条后、不再解析后面的重写条件)
            nginx内部引擎开始根据结果的rewrite结果寻找另一个位置匹配
            不再解析重写条件、即使在下一个位置匹配 (匹配之后、即使匹配结果包含rewrite、也不再进行解析)



 try_files $uri $uri/ /index.html; 
 第一步先查找有没有$uri的文件、如果没有、接着查找有没有$uri目录、如果也没有返回/index.html或发起子请求


URI: 统一资源标识符、用来标识唯一的资源、而URI包含两种:
1.URL： 统一资源定位器、描述了一台服务器上的某种资源的特定位置
2.URN： 统一资源名、是作为特定内容的唯一名称使用

Keepalived:
    是一个基于VRRP(虚拟路由冗余协议)协议来实现的服务高可用方案、可以认为它是实现路由器高可用的容错协议、即将N台提供相同功能的路由器组价成一个路由器组。

主要模块:
    core: 核心模块、负责主进程的启动、维护以及全局配置文件的加载和解析。、
    check: 负责健康检查、包括常见的各种检查方式。
    vrrp: 实现VRRP协议


Nginx 基于ip灰度发布、或者预发设置:
  - 首先定义2组upstream
    upstream prod {
        server x.x.x.x:8080;
        server x.x.x.x:8080;
    }
    upstream pre {
        server x.x.x.x:8080;
        server x.x.x.x:8080;
    } 
    set $web_backend prod; # 定义变量、后面根据ip来选择不同的后端
    if ($remote_addr ~ "1.1.1.1") { # 根据ip来进行灰度会预发
        set $web_backed pre;
    }  
    location / {
        proxy_pass http://$web_backend;
    }



NGINX 限流模块
  - ngx_http_limit_req_module  请求频率
    - limit_req_zone $binary_remote_addr zone=myRateLimit:10m rate=10r/s  10r/m
      1、binary_remote_addr 表示用客户端来限流
      2、zone 定义共享内存大小、
      3、rate 表示每秒最多出来10个请求

    - limit_req zone=name  burst=xx nodelay    
      1、burst 突发流量 是桶的最大容量
      2、nodelay 设置了nodelay burst会拒绝

    - limit_req zone=name  

limit_req  在 limit_conn 之前

  ngx_http_limit_conn_module 限制连接数
    - limit_conn_zone key zone=name:size; 
    - limit_conn_status 500; 超过时返回错误码
    - limit_conn_log_level error; 错误日志
    - 示例:
      limit_conn_zone $binary_remote_addr zone=addr:10m;

      server {
              listen 8080;
              root html;
              error_log logs/myerror.log info;

              location / {
                   limit_conn_status 500;
                   limit_conn_log_level info;
                   limit_rate 50;
                   limit_conn addr 1;
              }
      } 